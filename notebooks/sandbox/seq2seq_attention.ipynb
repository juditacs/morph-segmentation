{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model parameters\n",
    "\n",
    "Set `use_toy_data` to `True` for toy experiments. This will train the network on two unique examples.\n",
    "\n",
    "The real dataset is morphological reinflection task: Hungarian nouns in the instrumental case.\n",
    "Hungarian features both vowel harmony and assimilation.\n",
    "A few examples are listed here (capitalization is added for emphasis):\n",
    "\n",
    "| input | output | meaning | what happens |\n",
    "| :-----: | :-----: | :-----: | :-----: |\n",
    "| autó | autóval | with car | |\n",
    "| Peti | Petiv**E**l | with Pete | vowel harmony |\n",
    "| fej | fej**J**el | with head | assimilation |\n",
    "| pálca | pálc**Á**val | with stick | low vowel lengthening |\n",
    "| kulcs | kul**CCS**al | with key | digraph + assimilation |\n",
    "\n",
    "This turns out to be a very easy task for a fairly small seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"../../\"\n",
    "use_toy_data = False\n",
    "LOG_DIR = 'logs'  # Tensorboard log directory\n",
    "\n",
    "if use_toy_data:\n",
    "    batch_size = 8\n",
    "    embedding_dim = 5\n",
    "    cell_size = 32\n",
    "    max_len = 6\n",
    "else:\n",
    "    batch_size = 40\n",
    "    embedding_dim = 20\n",
    "    cell_size = 100\n",
    "    max_len = 33\n",
    "    \n",
    "use_attention = True\n",
    "use_bidirectional_encoder = True\n",
    "is_time_major = True\n",
    "do_beam_search = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Download data if necessary\n",
    "\n",
    "The input data is expected in the following format:\n",
    "\n",
    "~~~\n",
    "i n p u t 1 TAB o u t p u t 1\n",
    "i n p u t 2 TAB o u t p u t 2\n",
    "~~~\n",
    "\n",
    "Each line contains a single input-output pair separated by a TAB.\n",
    "Tokens are space-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if use_toy_data:\n",
    "    input_fn = 'toy_input.txt'\n",
    "    with open(input_fn, 'w') as f:\n",
    "        f.write('a b c\\td e f d e f\\n')\n",
    "        f.write('d e f\\ta b c a b c\\n')\n",
    "else:\n",
    "    DATA_DIR = '../../data/'\n",
    "    input_fn = 'instrumental.full.train'\n",
    "    input_fn = os.path.join(DATA_DIR, input_fn)\n",
    "    if not os.path.exists(input_fn):\n",
    "        import urllib\n",
    "        u = urllib.request.URLopener()\n",
    "        u.retrieve(\n",
    "            \"http://sandbox.mokk.bme.hu/~judit/resources/instrumental.full.train\", input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    PAD = 0\n",
    "    SOS = 1\n",
    "    EOS = 2\n",
    "    UNK = 3\n",
    "    #src_vocab = ['PAD', 'UNK']\n",
    "    constants = ['PAD', 'SOS', 'EOS', 'UNK']\n",
    "    hu_alphabet = list(\"aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz-+._\")\n",
    "    \n",
    "    def __init__(self, fn, config, src_alphabet=None, tgt_alphabet=None):\n",
    "        self.config = config\n",
    "        if hasattr(config, 'src_vocab'):\n",
    "            src_alphabet = list(config.src_vocab)\n",
    "            tgt_alphabet = list(config.tgt_vocab)\n",
    "        self.create_alphabets(src_alphabet, tgt_alphabet)\n",
    "        self.create_tables()\n",
    "        self.load_and_preproc_dataset(fn)\n",
    "        \n",
    "    def create_tables(self):\n",
    "        self.src_table = lookup_ops.index_table_from_tensor(\n",
    "            tf.constant(self.src_vocab), default_value=Dataset.UNK\n",
    "        )\n",
    "        if self.config.share_vocab:\n",
    "            self.tgt_table = self.src_table\n",
    "        else:\n",
    "            self.tgt_table = lookup_ops.index_table_from_tensor(\n",
    "                tf.constant(self.tgt_vocab), default_value=Dataset.UNK\n",
    "            )\n",
    "            \n",
    "    def create_alphabets(self, src_alphabet, tgt_alphabet):\n",
    "        if hasattr(self.config, 'src_vocab'):\n",
    "            self.src_vocab = Dataset.constants + list(self.config.src_vocab)\n",
    "            self.tgt_vocab = Dataset.constants + list(self.config.tgt_vocab)\n",
    "        else:\n",
    "            if src_alphabet is None:\n",
    "                self.src_vocab = Dataset.constants + Dataset.hu_alphabet\n",
    "            else:\n",
    "                self.src_vocab = Dataset.constants + alphabet\n",
    "            if self.config.share_vocab:\n",
    "                self.tgt_vocab = self.src_vocab\n",
    "                self.tgt_table = self.src_table\n",
    "            else:\n",
    "                if tgt_alphabet is None:\n",
    "                    self.tgt_vocab = Dataset.constants + Dataset.hu_alphabet\n",
    "                else:\n",
    "                    self.tgt_vocab = Dataset.constants + alphabet\n",
    "        self.src_vocab_size = len(self.src_vocab)\n",
    "        self.tgt_vocab_size = len(self.tgt_vocab)\n",
    "    \n",
    "    def load_and_preproc_dataset(self, fn):\n",
    "        dataset = tf.contrib.data.TextLineDataset(fn)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.map(lambda s: tf.string_split([s], delimiter='\\t').values)\n",
    "        \n",
    "        src = dataset.map(lambda s: s[0])\n",
    "        tgt = dataset.map(lambda s: s[1])\n",
    "        \n",
    "        src = src.map(lambda s: tf.string_split([s], delimiter=' ').values)\n",
    "        src = src.map(lambda s: s[:self.config.src_maxlen])\n",
    "        tgt = tgt.map(lambda s: tf.string_split([s], delimiter=' ').values)\n",
    "        tgt = tgt.map(lambda s: s[:self.config.tgt_maxlen])\n",
    "        \n",
    "        src = src.map(lambda words: self.src_table.lookup(words))\n",
    "        tgt = tgt.map(lambda words: self.tgt_table.lookup(words))\n",
    "        \n",
    "        dataset = tf.contrib.data.Dataset.zip((src, tgt))\n",
    "        dataset = dataset.map(\n",
    "            lambda src, tgt: (\n",
    "                src,\n",
    "                tf.concat(([Dataset.SOS], tgt), 0),\n",
    "                tf.concat((tgt, [Dataset.EOS]), 0),\n",
    "            )\n",
    "        )\n",
    "        dataset = dataset.map(\n",
    "            lambda src, tgt_in, tgt_out: (src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in))\n",
    "        )\n",
    "        batched = dataset.padded_batch(\n",
    "            self.config.batch_size,\n",
    "            padded_shapes=(\n",
    "                tf.TensorShape([self.config.src_maxlen]),\n",
    "                tf.TensorShape([self.config.tgt_maxlen+2]),\n",
    "                tf.TensorShape([None]),\n",
    "                tf.TensorShape([]),\n",
    "                tf.TensorShape([]),\n",
    "            )\n",
    "        )\n",
    "        self.batched_iter = batched.make_initializable_iterator()\n",
    "        s = self.batched_iter.get_next()\n",
    "        self.src_ids = s[0]\n",
    "        self.tgt_in_ids = s[1]\n",
    "        self.tgt_out_ids = s[2]\n",
    "        self.src_size = s[3]\n",
    "        self.tgt_size = s[4]\n",
    "        \n",
    "    def run_initializers(self, session):\n",
    "        session.run(tf.tables_initializer())\n",
    "        session.run(self.batched_iter.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create model\n",
    "\n",
    "## Embedding\n",
    "\n",
    "The input and output embeddings are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    default_fn = os.path.join(\n",
    "        PROJECT_DIR, \"config\", \"seq2seq\", \"default.yaml\"\n",
    "    )\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_defaults(fn=default_fn):\n",
    "        with open(fn) as f:\n",
    "            return yaml.load(f)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, fn):\n",
    "        params = yaml.load(fn)\n",
    "        return cls(**params)\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        defaults = Config.load_defaults()\n",
    "        for param, val in defaults.items():\n",
    "            setattr(self, param, val)\n",
    "        for param, val in kwargs.items():\n",
    "            setattr(self, param, val)\n",
    "            \n",
    "        if 'src_vocab' in kwargs:\n",
    "            self.load_vocabs()\n",
    "            \n",
    "    def load_vocabs(self):\n",
    "        with open(self.src_vocab) as f:\n",
    "            vocab = set(l.strip() for l in f)\n",
    "        self.src_vocab = vocab\n",
    "        with open(self.tgt_vocab) as f:\n",
    "            vocab = set(l.strip() for l in f)\n",
    "        self.tgt_vocab = vocab\n",
    "        \n",
    "input_fn = '/mnt/permanent/home/judit/projects/ulm/dat/sigmorphon2016/s2s_input/' \\\n",
    "    'hungarian-task2-train-standard'\n",
    "config = Config(src_maxlen=40, tgt_maxlen=33,\n",
    "                share_vocab=False,\n",
    "                src_vocab=input_fn+'.src_vocab',\n",
    "                tgt_vocab=input_fn+'.tgt_vocab')\n",
    "dataset = Dataset(input_fn, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"embedding\"):\n",
    "    embedding = tf.get_variable(\"embedding\", [dataset.src_vocab_size, embedding_dim], dtype=tf.float32)\n",
    "    embedding_input = tf.nn.embedding_lookup(embedding, dataset.src_ids)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(embedding, dataset.tgt_in_ids)\n",
    "    if is_time_major:\n",
    "        embedding_input = tf.transpose(embedding_input, [1, 0, 2])\n",
    "        decoder_emb_inp = tf.transpose(decoder_emb_inp, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"encoder\"):\n",
    "    \n",
    "    if use_bidirectional_encoder:\n",
    "        fw_cells = []\n",
    "        bw_cells = []\n",
    "        for i in range(1):\n",
    "            fw_cell = tf.nn.rnn_cell.BasicLSTMCell(cell_size)\n",
    "            fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, input_keep_prob=0.8)\n",
    "            fw_cells.append(fw_cell)\n",
    "            bw_cell = tf.nn.rnn_cell.BasicLSTMCell(cell_size)\n",
    "            bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, input_keep_prob=0.8)\n",
    "            bw_cells.append(bw_cell)\n",
    "            \n",
    "        fw_cell = tf.contrib.rnn.MultiRNNCell(fw_cells)\n",
    "        bw_cell = tf.contrib.rnn.MultiRNNCell(bw_cells)\n",
    "\n",
    "        o, e = tf.nn.bidirectional_dynamic_rnn(\n",
    "            fw_cell, bw_cell, embedding_input, dtype='float32', sequence_length=dataset.src_size,\n",
    "            time_major=is_time_major)\n",
    "        encoder_outputs = tf.concat(o, -1)\n",
    "        encoder_state = []\n",
    "        for i in range(1):\n",
    "            encoder_state.append(e[0][i])\n",
    "            encoder_state.append(e[1][i])\n",
    "        encoder_state = tuple(encoder_state)\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(cell_size)\n",
    "        fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, input_keep_prob=0.8)\n",
    "        o, e = tf.nn.dynamic_rnn(fw_cell, embedding_input, dtype='float32',\n",
    "                                 sequence_length=dataset.src_size, time_major=is_time_major)\n",
    "        encoder_outputs = o\n",
    "        encoder_state = e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\", dtype=\"float32\") as scope:\n",
    "    if use_bidirectional_encoder:\n",
    "        decoder_cells = []\n",
    "        for i in range(2):\n",
    "            decoder_cell = tf.contrib.rnn.BasicLSTMCell(cell_size)\n",
    "            decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=0.8)\n",
    "            decoder_cells.append(decoder_cell)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(decoder_cells)\n",
    "\n",
    "        if use_attention:\n",
    "            if is_time_major:\n",
    "                attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                attention_states = encoder_outputs\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                cell_size, attention_states, memory_sequence_length=dataset.src_size,\n",
    "                scale=True\n",
    "            )\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell, attention_mechanism, attention_layer_size=cell_size,\n",
    "                name=\"attention\"\n",
    "            )\n",
    "            if is_time_major:\n",
    "                decoder_initial_state = decoder_cell.zero_state(\n",
    "                    tf.shape(decoder_emb_inp)[1], tf.float32).clone(cell_state=encoder_state)\n",
    "            else:\n",
    "                decoder_initial_state = decoder_cell.zero_state(\n",
    "                    tf.shape(decoder_emb_inp)[0], tf.float32).clone(cell_state=encoder_state)\n",
    "        else:\n",
    "            decoder_initial_state = encoder_state\n",
    "            \n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.BasicLSTMCell(cell_size)\n",
    "        decoder_initial_state = encoder_state\n",
    "        \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        decoder_emb_inp, dataset.tgt_size, time_major=is_time_major)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, decoder_initial_state)\n",
    "    \n",
    "    outputs, final, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, output_time_major=is_time_major, swap_memory=True, scope=scope)\n",
    "    \n",
    "    output_proj = layers_core.Dense(dataset.tgt_vocab_size, name=\"output_proj\")\n",
    "    logits = output_proj(outputs.rnn_output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loss and training operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"train\"):\n",
    "    if is_time_major:\n",
    "        logits = tf.transpose(logits, [1, 0, 2])\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=dataset.tgt_out_ids, logits=logits)\n",
    "        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n",
    "    else:\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=dataset.tgt_out_ids, logits=logits)\n",
    "        target_weights = tf.sequence_mask(dataset.tgt_size, tf.shape(logits)[1], tf.float32)\n",
    "    loss = tf.reduce_sum(crossent * target_weights) / tf.to_float(batch_size)\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
    "    max_global_norm = tf.placeholder(dtype=tf.float32, name=\"max_global_norm\")\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.5)\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss, params)\n",
    "    for grad, var in zip(gradients, params):\n",
    "        tf.summary.histogram(var.op.name+'/gradient', grad)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, max_global_norm)\n",
    "    for grad, var in zip(gradients, params):\n",
    "        tf.summary.histogram(var.op.name+'/clipped_gradient', grad)\n",
    "    update = optimizer.apply_gradients(zip(gradients, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Greedy decoder for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"greedy_decoder\"):\n",
    "    g_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        embedding, tf.fill([dataset.config.batch_size], dataset.SOS), dataset.EOS)\n",
    "    g_decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, g_helper, decoder_initial_state,\n",
    "                                             output_layer=output_proj)\n",
    "\n",
    "    g_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(g_decoder, maximum_iterations=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Beam search decoder\n",
    "\n",
    "Attention + beam search doesn't work yet :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if do_beam_search:\n",
    "    with tf.variable_scope(\"beam_search\"):\n",
    "        beam_width = 4\n",
    "        start_tokens = tf.fill([config.batch_size], dataset.SOS)\n",
    "        if use_attention:\n",
    "            bm_dec_initial_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                tf.contrib.seq2seq.tile_batch(encoder_state[0], multiplier=beam_width),\n",
    "                tf.contrib.seq2seq.tile_batch(encoder_state[1], multiplier=beam_width)\n",
    "            )\n",
    "        else:\n",
    "            bm_dec_initial_state = tf.contrib.seq2seq.tile_batch(\n",
    "                encoder_state, multiplier=beam_width)\n",
    "        bm_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=decoder_cell,\n",
    "            embedding=embedding,\n",
    "            start_tokens=start_tokens,\n",
    "            initial_state=bm_dec_initial_state,\n",
    "            beam_width=beam_width,\n",
    "            output_layer=output_proj,\n",
    "            end_token=dataset.EOS,\n",
    "        )\n",
    "        bm_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            bm_decoder, maximum_iterations=config.tgt_maxlen,\n",
    "            impute_finished=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Starting session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "sess = tf.Session()\n",
    "dataset.run_initializers(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 's2s_sandbox', 'tmp'))\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Running 3000 epochs with learning rate 1\n",
      "Iter 100, learning rate 1, loss 61.96318435668945\n",
      "Iter 200, learning rate 1, loss 47.77288055419922\n",
      "Iter 300, learning rate 1, loss 38.960784912109375\n",
      "Iter 400, learning rate 1, loss 17.833261489868164\n",
      "Iter 500, learning rate 1, loss 12.189311981201172\n",
      "Iter 600, learning rate 1, loss 13.220904350280762\n",
      "Iter 700, learning rate 1, loss 11.429954528808594\n",
      "Iter 800, learning rate 1, loss 12.432929992675781\n",
      "Iter 900, learning rate 1, loss 12.960041999816895\n",
      "Iter 1000, learning rate 1, loss 13.193499565124512\n",
      "Iter 1100, learning rate 1, loss 28.667856216430664\n",
      "Iter 1200, learning rate 1, loss 11.161737442016602\n",
      "Iter 1300, learning rate 1, loss 20.340717315673828\n",
      "Iter 1400, learning rate 1, loss 16.091922760009766\n",
      "Iter 1500, learning rate 1, loss 18.222150802612305\n",
      "Iter 1600, learning rate 1, loss 14.37237548828125\n",
      "Iter 1700, learning rate 1, loss 13.986806869506836\n",
      "Iter 1800, learning rate 1, loss 13.638595581054688\n",
      "Iter 1900, learning rate 1, loss 9.2158203125\n",
      "Iter 2000, learning rate 1, loss 8.632148742675781\n",
      "Iter 2100, learning rate 1, loss 18.224632263183594\n",
      "Iter 2200, learning rate 1, loss 13.65808391571045\n",
      "Iter 2300, learning rate 1, loss 11.184611320495605\n",
      "Iter 2400, learning rate 1, loss 24.059162139892578\n",
      "Iter 2500, learning rate 1, loss 16.134634017944336\n",
      "Iter 2600, learning rate 1, loss 19.908954620361328\n",
      "Iter 2700, learning rate 1, loss 23.231597900390625\n",
      "Iter 2800, learning rate 1, loss 16.443878173828125\n",
      "Iter 2900, learning rate 1, loss 16.652585983276367\n",
      "Iter 3000, learning rate 1, loss 14.974614143371582\n",
      "Running 3000 epochs with learning rate 0.1\n",
      "Iter 100, learning rate 0.1, loss 9.538423538208008\n",
      "Iter 200, learning rate 0.1, loss 7.053174018859863\n",
      "Iter 300, learning rate 0.1, loss 7.999828338623047\n",
      "Iter 400, learning rate 0.1, loss 5.259898662567139\n",
      "Iter 500, learning rate 0.1, loss 6.7113776206970215\n",
      "Iter 600, learning rate 0.1, loss 4.774209022521973\n",
      "Iter 700, learning rate 0.1, loss 3.960294246673584\n",
      "Iter 800, learning rate 0.1, loss 4.148555755615234\n",
      "Iter 900, learning rate 0.1, loss 4.06333065032959\n",
      "Iter 1000, learning rate 0.1, loss 3.03228759765625\n",
      "Iter 1100, learning rate 0.1, loss 4.243378639221191\n",
      "Iter 1200, learning rate 0.1, loss 3.891500949859619\n",
      "Iter 1300, learning rate 0.1, loss 4.473636150360107\n",
      "Iter 1400, learning rate 0.1, loss 3.2054531574249268\n",
      "Iter 1500, learning rate 0.1, loss 2.9399900436401367\n",
      "Iter 1600, learning rate 0.1, loss 4.90584659576416\n",
      "Iter 1700, learning rate 0.1, loss 3.2475154399871826\n",
      "Iter 1800, learning rate 0.1, loss 2.83796763420105\n",
      "Iter 1900, learning rate 0.1, loss 2.9384922981262207\n",
      "Iter 2000, learning rate 0.1, loss 2.808964967727661\n",
      "Iter 2100, learning rate 0.1, loss 2.4553215503692627\n",
      "Iter 2200, learning rate 0.1, loss 2.3363049030303955\n",
      "Iter 2300, learning rate 0.1, loss 2.725834608078003\n",
      "Iter 2400, learning rate 0.1, loss 2.4920780658721924\n",
      "Iter 2500, learning rate 0.1, loss 2.782288074493408\n",
      "Iter 2600, learning rate 0.1, loss 2.9779820442199707\n",
      "Iter 2700, learning rate 0.1, loss 2.7303810119628906\n",
      "Iter 2800, learning rate 0.1, loss 2.227452516555786\n",
      "Iter 2900, learning rate 0.1, loss 2.2211709022521973\n",
      "Iter 3000, learning rate 0.1, loss 2.655975103378296\n",
      "Running 10000 epochs with learning rate 0.01\n",
      "Iter 100, learning rate 0.01, loss 1.3175971508026123\n",
      "Iter 200, learning rate 0.01, loss 2.1053829193115234\n",
      "Iter 300, learning rate 0.01, loss 2.0980608463287354\n",
      "Iter 400, learning rate 0.01, loss 1.283769965171814\n",
      "Iter 500, learning rate 0.01, loss 4.167130470275879\n",
      "Iter 600, learning rate 0.01, loss 3.0038132667541504\n",
      "Iter 700, learning rate 0.01, loss 2.1849489212036133\n",
      "Iter 800, learning rate 0.01, loss 1.8061408996582031\n",
      "Iter 900, learning rate 0.01, loss 2.35823392868042\n",
      "Iter 1000, learning rate 0.01, loss 3.1702487468719482\n",
      "Iter 1100, learning rate 0.01, loss 1.9911291599273682\n",
      "Iter 1200, learning rate 0.01, loss 1.543060541152954\n",
      "Iter 1300, learning rate 0.01, loss 1.6987390518188477\n",
      "Iter 1400, learning rate 0.01, loss 2.1293764114379883\n",
      "Iter 1500, learning rate 0.01, loss 3.262300491333008\n",
      "Iter 1600, learning rate 0.01, loss 2.2577738761901855\n",
      "Iter 1700, learning rate 0.01, loss 2.3905868530273438\n",
      "Iter 1800, learning rate 0.01, loss 2.1316282749176025\n",
      "Iter 1900, learning rate 0.01, loss 2.4206411838531494\n",
      "Iter 2000, learning rate 0.01, loss 1.4855680465698242\n",
      "Iter 2100, learning rate 0.01, loss 1.495945930480957\n",
      "Iter 2200, learning rate 0.01, loss 1.6354963779449463\n",
      "Iter 2300, learning rate 0.01, loss 2.989838123321533\n",
      "Iter 2400, learning rate 0.01, loss 2.0234291553497314\n",
      "Iter 2500, learning rate 0.01, loss 2.260322332382202\n",
      "Iter 2600, learning rate 0.01, loss 2.2149434089660645\n",
      "Iter 2700, learning rate 0.01, loss 2.663991928100586\n",
      "Iter 2800, learning rate 0.01, loss 2.1905531883239746\n",
      "Iter 2900, learning rate 0.01, loss 2.8410465717315674\n",
      "Iter 3000, learning rate 0.01, loss 2.3754031658172607\n",
      "Iter 3100, learning rate 0.01, loss 2.649921178817749\n",
      "Iter 3200, learning rate 0.01, loss 2.7100863456726074\n",
      "Iter 3300, learning rate 0.01, loss 1.814025640487671\n",
      "Iter 3400, learning rate 0.01, loss 1.8607614040374756\n",
      "Iter 3500, learning rate 0.01, loss 3.2710578441619873\n",
      "Iter 3600, learning rate 0.01, loss 2.493704080581665\n",
      "Iter 3700, learning rate 0.01, loss 2.944732666015625\n",
      "Iter 3800, learning rate 0.01, loss 2.3235371112823486\n",
      "Iter 3900, learning rate 0.01, loss 2.4231228828430176\n",
      "Iter 4000, learning rate 0.01, loss 1.6433887481689453\n",
      "Iter 4100, learning rate 0.01, loss 1.909977674484253\n",
      "Iter 4200, learning rate 0.01, loss 2.2363994121551514\n",
      "Iter 4300, learning rate 0.01, loss 1.9865280389785767\n",
      "Iter 4400, learning rate 0.01, loss 2.3437600135803223\n",
      "Iter 4500, learning rate 0.01, loss 2.341573476791382\n",
      "Iter 4600, learning rate 0.01, loss 2.4271655082702637\n",
      "Iter 4700, learning rate 0.01, loss 2.623760223388672\n",
      "Iter 4800, learning rate 0.01, loss 2.46822190284729\n",
      "Iter 4900, learning rate 0.01, loss 1.483943223953247\n",
      "Iter 5000, learning rate 0.01, loss 2.61393404006958\n",
      "Iter 5100, learning rate 0.01, loss 2.129786729812622\n",
      "Iter 5200, learning rate 0.01, loss 2.2490651607513428\n",
      "Iter 5300, learning rate 0.01, loss 1.6323308944702148\n",
      "Iter 5400, learning rate 0.01, loss 1.10919189453125\n",
      "Iter 5500, learning rate 0.01, loss 2.235016107559204\n",
      "Iter 5600, learning rate 0.01, loss 2.237349271774292\n",
      "Iter 5700, learning rate 0.01, loss 1.3522922992706299\n",
      "Iter 5800, learning rate 0.01, loss 1.8546432256698608\n",
      "Iter 5900, learning rate 0.01, loss 1.7571172714233398\n",
      "Iter 6000, learning rate 0.01, loss 1.3410464525222778\n",
      "Iter 6100, learning rate 0.01, loss 1.7432225942611694\n",
      "Iter 6200, learning rate 0.01, loss 2.021531581878662\n",
      "Iter 6300, learning rate 0.01, loss 2.1403956413269043\n",
      "Iter 6400, learning rate 0.01, loss 3.3339362144470215\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def train(epochs, logstep, lr):\n",
    "    print(\"Running {} epochs with learning rate {}\".format(epochs, lr))\n",
    "    for i in range(epochs):\n",
    "        _, s = sess.run([update, merged_summary], feed_dict={learning_rate: lr, max_global_norm: 5.0})\n",
    "        l = sess.run(loss)\n",
    "        writer.add_summary(s, i)\n",
    "        if i % logstep == logstep - 1:\n",
    "            print(\"Iter {}, learning rate {}, loss {}\".format(i+1, lr, l))\n",
    "            \n",
    "print(\"Start training...\")\n",
    "if use_toy_data:\n",
    "    train(100, 10, .5)\n",
    "else:\n",
    "    train(3000, 100, 1)\n",
    "    train(3000, 100, 0.1)\n",
    "    train(10000, 100, 0.01)\n",
    "    #train(10000, 100, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "src_inv_vocab = {i: v for i, v in enumerate(dataset.src_vocab)}\n",
    "tgt_inv_vocab = {i: v for i, v in enumerate(dataset.tgt_vocab)}\n",
    "skip_symbols = ('PAD',)\n",
    "\n",
    "def decode_ids(input_ids, output_ids):\n",
    "    decoded = []\n",
    "    for sample_i in range(output_ids.shape[0]):\n",
    "        input_sample = input_ids[sample_i]\n",
    "        output_sample = output_ids[sample_i]\n",
    "        input_decoded = [src_inv_vocab[s] for s in input_sample]\n",
    "        input_decoded = ''.join(c for c in input_decoded if c not in skip_symbols)\n",
    "        output_decoded = [tgt_inv_vocab[s] for s in output_sample]\n",
    "        try:\n",
    "            eos_idx = output_decoded.index('EOS')\n",
    "        except ValueError:  # EOS not in list\n",
    "            eos_idx = len(output_decoded)\n",
    "        output_decoded = output_decoded[:eos_idx]\n",
    "        output_decoded = ''.join(c for c in output_decoded if c not in skip_symbols)\n",
    "        decoded.append((input_decoded, output_decoded))\n",
    "    return decoded\n",
    "\n",
    "if do_beam_search:\n",
    "    input_ids, output_ids, bm_output_ids = sess.run([dataset.src_ids, g_outputs.sample_id,\n",
    "                                                    bm_outputs.predicted_ids])\n",
    "else:\n",
    "    input_ids, output_ids = sess.run([dataset.src_ids, g_outputs.sample_id])\n",
    "decoded = decode_ids(input_ids, output_ids)\n",
    "print('\\n'.join(\n",
    "    '{} ---> {}'.format(dec[0], dec[1]) for dec in decoded\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if do_beam_search:\n",
    "    all_decoded = []\n",
    "    for beam_i in range(beam_width):\n",
    "        inputs = []\n",
    "        all_decoded.append([])\n",
    "        decoded = decode_ids(input_ids, bm_output_ids[:,:,beam_i])\n",
    "        for dec in decoded:\n",
    "            all_decoded[-1].append(dec[1])\n",
    "            inputs.append(dec[0])\n",
    "\n",
    "    print('\\n'.join(\n",
    "        '{} ---> {}'.format(inputs[i], ' / '.join(d[i] for d in all_decoded))\n",
    "                            for i in range(len(inputs))\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
